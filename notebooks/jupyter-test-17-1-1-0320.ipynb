{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 프로젝트 : 네이버 영화리뷰 감성분석 도전하기"],"metadata":{"id":"_kNZEFwMb2HL"}},{"cell_type":"markdown","source":["이전 스텝까지는 영문 텍스트의 감정 분석을 진행해 보았습니다. 그렇다면 이번에는 한국어 텍스트의 감정 분석을 진행해 보면 어떨까요? 오늘 활용할 데이터셋은 네이버 영화의 댓글을 모아 구성된 [Naver sentiment movie corpus](https://github.com/e9t/nsmc)입니다.\n","\n","아래와 같이 다운로드를 진행해 주세요."],"metadata":{"id":"S9GnobkScXCb"}},{"cell_type":"markdown","source":["\n","\n","```\n","$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n","$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n","$ mv ratings_*.txt ~/aiffel/sentiment_classification/data\n","$ pip install konlpy\n","$ sudo apt-get install git\n","$ sudo apt-get install curl\n","$ bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n","\n","```\n","\n","프로젝트 진행 순서는 다음과 같습니다.\n","\n"],"metadata":{"id":"kT_t99LYckcj"}},{"cell_type":"markdown","source":["## 라이브러리 버전을 확인해 봅니다\n","---\n","사용할 라이브러리 버전을 둘러봅시다."],"metadata":{"id":"S9OoRDxPcrOL"}},{"cell_type":"code","source":["import pandas\n","import konlpy\n","import gensim\n","\n","print(pandas.__version__)\n","print(konlpy.__version__)\n","print(gensim.__version__)"],"metadata":{"id":"Jkc7NKMgcq7r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1) 데이터 준비와 확인\n","---"],"metadata":{"id":"zNIpAesGcvgx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuTiJUcjbxc0"},"outputs":[],"source":["import pandas as pd\n","\n","# 데이터를 읽어봅시다.\n","train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n","test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n","\n","train_data.head()"]},{"cell_type":"markdown","source":["## 2) 데이터로더 구성\n","---\n","실습 때 다루었던 IMDB 데이터셋은 텍스트를 가공하여 `imdb.data_loader()` 메서드를 호출하면 숫자 인덱스로 변환된 텍스트와 `word_to_index` 딕셔너리까지 친절하게 제공합니다. 그러나 이번에 다루게 될 nsmc 데이터셋은 전혀 가공되지 않은 텍스트 파일로 이루어져 있습니다. 이것을 읽어서 `imdb.data_loader()`와 동일하게 동작하는 자신만의 `data_loader`를 만들어 보는 것으로 시작합니다. `data_loader` 안에서는 다음을 수행해야 합니다.\n","\n","- 데이터의 중복 제거\n","- NaN 결측치 제거\n","- 한국어 토크나이저로 토큰화\n","- 불용어(Stopwords) 제거\n","- 사전`word_to_index` 구성\n","- 텍스트 스트링을 사전 인덱스 스트링으로 변환\n","- `X_train`, `y_train`, `X_test`, `y_test`, `word_to_index` 리턴"],"metadata":{"id":"CEEDjGcgczdZ"}},{"cell_type":"code","source":["from konlpy.tag import Mecab\n","import numpy as np\n","from collections import Counter\n","\n","tokenizer = Mecab()\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","\n","def load_data(train_data, test_data, num_words=num_words):\n","    # [[YOUR CODE]]\n","\n","X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"],"metadata":{"id":"pAxqupKDdCWG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index_to_word = {index:word for word, index in word_to_index.items()}"],"metadata":{"id":"hhUGwgsQdFlp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다.\n","# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다.\n","def get_encoded_sentence(sentence, word_to_index):\n","    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n","\n","# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다.\n","def get_encoded_sentences(sentences, word_to_index):\n","    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n","\n","# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다.\n","def get_decoded_sentence(encoded_sentence, index_to_word):\n","    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n","\n","# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다.\n","def get_decoded_sentences(encoded_sentences, index_to_word):\n","    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"],"metadata":{"id":"TqaQmmVldGbV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3) 모델 구성을 위한 데이터 분석 및 가공\n","---\n","- 데이터셋 내 문장 길이 분포\n","- 적절한 최대 문장 길이 지정\n","- keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가"],"metadata":{"id":"Z9lC1tkhdH8a"}},{"cell_type":"markdown","source":["## 4) 모델 구성 및 validation set 구성\n","---\n","모델은 3가지 이상 다양하게 구성하여 실험해 보세요."],"metadata":{"id":"Tsx5borudQT-"}},{"cell_type":"markdown","source":["## 5) 모델 훈련 개시\n","---"],"metadata":{"id":"WH9h3ruNdST0"}},{"cell_type":"markdown","source":["## 6) Loss, Accuracy 그래프 시각화\n","---"],"metadata":{"id":"zrNWgRpWdTZz"}},{"cell_type":"markdown","source":["## 7) 학습된 Embedding 레이어 분석\n","---"],"metadata":{"id":"GF82ZuIFdVD9"}},{"cell_type":"markdown","source":["## 8) 한국어 Word2Vec 임베딩 활용하여 성능 개선\n","---\n","- 한국어 Word2Vec은 /data 폴더 안에 있는 word2vec_ko.model을 활용하세요.\n","- 한국어 Word2Vec을 활용할 때는 `load_word2vec_format()` 형태가 아닌 `load()` 형태로 모델을 불러와주세요. 또한 모델을 활용할 때에는 아래 예시와 같이 `.wv`를 붙여서 활용합니다. 좀더 자세한 활용법에 대해선 다음 링크들을 참조해주세요. [참고 링크1](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#storing-and-loading-models), [참고 링크2](https://radimrehurek.com/gensim/models/keyedvectors.html)"],"metadata":{"id":"ZXppU80Vdd4d"}},{"cell_type":"markdown","source":["\n","\n","```\n","# 예시 코드\n","from gensim.models.keyedvectors import Word2VecKeyedVectors\n","word_vectors = Word2VecKeyedVectors.load(word2vec_file_path)\n","vector = word_vectors.wv[‘끝’]\n","```\n","\n"],"metadata":{"id":"r6Y9ovkzdqcm"}}]}